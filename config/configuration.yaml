# DreamerV3 Configuration
# Defines settings for training, environment, model architecture, and optimization.

# General settings for training
precision: 16  # Mixed precision training for GPU efficiency (16-bit)
computation_device: "cuda"  # Preferred device (falls back to CPU if CUDA unavailable)
random_seed: 42  # Seed for reproducibility across libraries
action_repeat: 1  # Number of times to repeat actions (1 for CartPole-v1)
total_training_steps: 1000000  # Total environment steps for training (1M per DreamerV3)
evaluation_interval_steps: 10000  # Steps between evaluations (adjusted for longer training)
evaluation_number_of_episodes: 10  # Episodes to run during evaluation
training_episode_directory: "episodes/train"  # Directory for training episode data
evaluation_episode_directory: "episodes/eval"  # Directory for evaluation episode data
batch_size: 16  # Number of sequences per training batch
sequence_length: 50  # Length of sequences sampled from the dataset
number_of_environments: 1  # Single environment (sequential execution)
compile_models: False  # Disable Torch compilation if compatibility issues arise
enable_debugging: False  # Enable verbose logging for debugging
buffer_capacity: 1000000  # Maximum steps stored in the replay buffer
minimum_steps_to_start_training: 1000  # Steps required before training begins
prefill_steps: 1000  # Steps to prefill the buffer with random actions
steps_per_iteration: 1  # Environment steps per training iteration (adjusted for high train_ratio)
loss_print_interval: 100  # Interval for printing loss changes

# Training Parameters
train_ratio: 32  # High training ratio (updates per env step) per DreamerV3, was 512 official
training_updates_per_step: 8  # Base updates per step (overridden by train_ratio)
use_deterministic_mode: True  # Enable deterministic operations for reproducibility
gradient_check_frequency: 100  # Frequency of gradient visualization for debugging
exploration_termination_step: 50000  # Steps for random exploration phase

# Logging
logging_interval: 100  # Log metrics every 100 steps
log_video_predictions: True  # Generate and log video predictions during evaluation

# Optimizer Settings
optimizer_type: "adam"  # Optimizer type (Adam as per official DreamerV3)
model_learning_rate: 1e-4  # Learning rate for the world model
optimizer_epsilon: 1e-8  # Epsilon for numerical stability in Adam
gradient_clip_value: 1000.0  # Gradient clipping for world model stability (DreamerV3 default)
weight_decay_value: 0.0  # No weight decay (official setting)

# Actor Settings
actor_number_of_layers: 5  # Increased to 5 per DreamerV3 recommendation
actor_distribution_type: "onehot"  # Distribution for discrete actions (CartPole-v1)
actor_lr: 3e-5  # Learning rate for the actor
actor_eps: 1e-8  # Epsilon for actor optimizer stability
actor_grad_clip: 1000.0  # Gradient clipping for actor stability (DreamerV3 default)
actor_entropy: 0.01  # Adjusted entropy coefficient for exploration (from 3e-4)
actor_temperature: 1.0  # Temperature for action distribution
actor_unimix_ratio: 0.01  # Uniform mixing ratio for exploration (DreamerV3 default)
actor_exploration_behavior: "random"  # Initial exploration uses random policy
actor_imag_gradient: "reinforce"  # Gradient method for imagined trajectories (discrete)

# Critic Settings
critic_number_of_layers: 5  # Increased to 5 per DreamerV3 recommendation
critic_lr: 3e-5  # Learning rate for the critic
critic_eps: 1e-8  # Epsilon for critic optimizer stability
critic_grad_clip: 1000.0  # Gradient clipping for critic stability (DreamerV3 default)
critic_use_slow_target: True  # Use a slow-moving target network
critic_slow_target_update_interval: 1  # Update target every step
critic_slow_target_update_fraction: 0.005  # Slower Polyak averaging factor (DreamerV3 default)

# World Model Settings
encoder_output_dimension: 1024  # Size of the encoder's output embedding
encoder_number_of_layers: 5  # Increased to 5 per DreamerV3 CNN spec
dynamics_stochastic_dimension: 32  # Size of the stochastic latent state
dynamics_deterministic_dimension: 512  # Size of the deterministic latent state, was 4096 official
dynamics_hidden_units: 1024  # Hidden units in the GRU dynamics model
dynamics_recurrent_depth: 1  # Single-layer GRU
dynamics_use_discrete: True  # Use discrete latent representations
discrete_latent_num: 32  # Number of categorical variables
discrete_latent_size: 32  # Number of classes per variable
unimix_ratio: 0.01  # Uniform mixing ratio for distributions (DreamerV3 default)
dynamics_loss_scale: 0.5  # KL loss scale for dynamics term (DreamerV3 default)
representation_loss_scale: 0.1  # KL loss scale for representation term (DreamerV3 default)
kl_free: 1.0  # Free nats for KL balancing

# Decoder and Encoder
decoder_loss_scale: 1.0  # Loss weight for observation reconstruction
decoder_number_of_layers: 4  # Number of convolutional layers in the decoder (DreamerV3 default)
decoder_hidden_dimension: 512  # Hidden dimension for decoder conv layers (matches units)

# Prediction Heads
reward_head_number_of_layers: 5  # Increased to 5 per DreamerV3 recommendation
reward_head_loss_scale: 0.5  # Loss weight for reward prediction (DreamerV3 default)
reward_clipping_value: 0.0  # No reward clipping (handled by symlog per DreamerV3)
reward_head_temperature: 1.0  # Temperature for reward distribution
reward_head_unimix_ratio: 0.01  # Uniform mixing ratio for reward (DreamerV3 default)
continuation_head_number_of_layers: 5  # Increased to 5 per DreamerV3 recommendation
continuation_head_loss_scale: 0.1  # Loss weight for continuation prediction (DreamerV3 default)
continuation_head_temperature: 1.0  # Temperature for continuation distribution
continuation_head_unimix_ratio: 0.01  # Uniform mixing ratio for continuation (DreamerV3 default)

# Network Settings
units: 128  # Default hidden units for MLPs, official 512
activation_function: "silu"  # SiLU/Swish activation function
normalization_type: "layer"  # Layer normalization
use_orthogonal_initialization: True  # Orthogonal initialization for weights

# Training Hyperparameters
discount_factor: 0.997  # Discount factor for future rewards
discount_lambda: 0.95  # Lambda for lambda-return computation
imag_horizon: 15  # Steps to imagine forward
use_reward_EMA: True  # Enable exponential moving average for reward normalization
return_ema_alpha: 0.01  # Update rate for return EMA
reward_ema_alpha: 0.01  # Update rate for reward EMA
max_samples_per_epoch: 10000  # Maximum samples per training epoch

# Data Augmentation
augmentation_enabled: True  # Enable image data augmentation
augmentation_crop_size: 64  # Crop size for augmentation
dataset_augmentation: True  # Enable dataset-level augmentation

# Replay Buffer
use_priority_replay: False  # Optional prioritized replay (currently uniform)

# Environment
image_size: 64  # Size of observation images (square)
number_of_workers: 0  # Number of DataLoader workers (0 for single-threaded)

# CartPole-Specific Settings
task_name: "CartPole-v1"  # Environment name